[{"content":"In this series of posts we\u0026rsquo;re going to build a text classification model that predicts whether a comment expresses a negative or positive stance on COVID-19 vaccination. This short first installment will deal with the way in which we digest the raw comments into something a bit more workable before doing any machine-learn-y things.\nCleaning up the comments The data we have on hand are roughly 9,000 scraped comments from various sources such as YouTube, Reddit and Twitter. Each comment has first been labeled individually as either 1 or 0, expressing a positive or negative stance respectively. All comments have after that also independently been labeled by at least one other annotator, as we will see. Let\u0026rsquo;s start up pandas, our data handling package of choice, and inspect some rows!\nimport pandas as pd\r# These comments are pretty long\rpd.set_option('display.max_colwidth', None)\r# Technically not a csv but pandas don't mind\rdf = pd.read_csv('vacc_train_data.tsv', sep='\\t', names=['target','comment'])\rfor row in df.loc[:5,'comment']:\rprint('\u0026quot;' + row + '\u0026quot;\\n')\r\u0026quot; It is easier to fool a million people than it is to convince a million people that they have been fooled. - Mark Twain\u0026quot;\r\u0026quot; NATURAL IMMUNITY protected us since evolution. Do not exist anymore?\u0026quot;\r\u0026quot; NATURAL IMMUNITY protected us since evolution. Do not exist anymore? ? ? No one talks about it, Why? ?\u0026quot;\r\u0026quot; The bigest sideffect of vaccines is fewer dead children That is savage\u0026quot;\r\u0026quot;90% of people that get vaccinated don't get the virus Wooow what's in the vaccine then? I'm very pro vaccination, but in my opinion Covid 19 vaccine is just sweetened water.\u0026quot;\r\u0026quot;95.6% effective against the original strain and 85.6% effective against the variant. Excellent news. Every positive news during this pandemic is to be welcomed (no pun intended)\u0026quot;\r The goal of our preprocessing steps here is to homogenize the text data a bit. We will later in our first model only consider the words in a given sentence, without regarding the order in which they were written. To this end, punctuation becomes very irrelevant so we will just strip it, and we will also make sure that everything is in lower case. Website URLs also seem reasonable to remove.\nEmojis Special characters that however are of interest are emojis. Consider the following three fictitious comments:\n Vaccine = â˜  vaccine ðŸ˜€ ðŸ’‰ðŸ¤®ðŸ¤®  Stripping the emojis from these comments removes pretty important information. Luckily for us, all emojis have descriptions and we can just translate these using the demoji package! We can now write the text cleaning function. We will make use of some regexp-stuff, just roll with the punches if this looks weird.\nimport re, string\rimport demoji\r#demoji.download_codes()\rdef textify_emojis(text):\r# Returns a dictionary with 'emoji' : 'description' pairs\remojis = demoji.findall(text)\r# We have to slightly modify the descriptions\r# by replacing all special characters with dashes\r# and then surrounding them with \u0026quot;:\u0026quot; and whitespace\rfor emoji, desc in emojis.items():\rdesc = ' :'+re.sub(r\u0026quot;[^0-9a-zA-Z]+\u0026quot;, \u0026quot;-\u0026quot;, desc )+': '\rtext = re.sub(emoji, desc, text)\rreturn text\r# Function that returns nice and clean text\rdef clean_text(text):\r# Make all text lowercase\rtext = text.lower()\r# Remove links\rtext = re.sub('https?://\\S+|www\\.\\S+', '', text)\r# Replace newline with space\rtext = re.sub('\\n', ' ', text)\r# Remove all \u0026quot;:\u0026quot; and \u0026quot;-\u0026quot;-characters first, we will # use these to represent emojis\rtext = re.sub(r\u0026quot;[:\\-]\u0026quot;, '', text)\r# Replace emojis with text descriptions\rtext = textify_emojis(text)\r# Remove most non-alphanumerical characters\rtext = re.sub(r\u0026quot;[^0-9a-zA-Z%:\\- ]+\u0026quot;, \u0026quot;\u0026quot;, text)\r# Get rid of unnecessary whitespace\rtext = ' '.join(text.split())\r# Done!\rreturn text text = \u0026quot;ðŸ’‰ðŸ¤®ðŸ¤®\u0026quot;\rprint(text, '---\u0026gt;', '\u0026quot;' + clean_text(text) + '\u0026quot;')\rðŸ’‰ðŸ¤®ðŸ¤® ---\u0026gt; \u0026quot;:syringe: :face-vomiting: :face-vomiting:\u0026quot;\r Nice! Now we apply this to the whole dataset.\ndf['comment'] = df['comment'].apply(lambda x: clean_text(x))\rfor row in df.loc[:5,'comment']:\rprint('\u0026quot;' + row + '\u0026quot;\\n')\r\u0026quot;it is easier to fool a million people than it is to convince a million people that they have been fooled mark twain\u0026quot;\r\u0026quot;natural immunity protected us since evolution do not exist anymore\u0026quot;\r\u0026quot;natural immunity protected us since evolution do not exist anymore no one talks about it why\u0026quot;\r\u0026quot;the bigest sideffect of vaccines is fewer dead children that is savage\u0026quot;\r\u0026quot;90% of people that get vaccinated dont get the virus wooow whats in the vaccine then im very pro vaccination but in my opinion covid 19 vaccine is just sweetened water\u0026quot;\r\u0026quot;956% effective against the original strain and 856% effective against the variant excellent news every positive news during this pandemic is to be welcomed no pun intended\u0026quot;\r Okay, the percentages look a bit wonky, and we didn\u0026rsquo;t see any example with an encoded emoji. Looking at the raw data, row 36 contains a comment with an emoji.\nprint('\u0026quot;' + df.loc[35,'comment'] + '\u0026quot;')\rtrust the government :rolling-on-the-floor-laughing:\r We might do something about the percentages later, but that\u0026rsquo;s all for now!\n","permalink":"http://vincentmolin.github.io/posts/vacc-part1-preprocess/","summary":"In this series of posts we\u0026rsquo;re going to build a text classification model that predicts whether a comment expresses a negative or positive stance on COVID-19 vaccination. This short first installment will deal with the way in which we digest the raw comments into something a bit more workable before doing any machine-learn-y things.\nCleaning up the comments The data we have on hand are roughly 9,000 scraped comments from various sources such as YouTube, Reddit and Twitter.","title":"Sentiment Classification: Preprocessing Part 1"},{"content":"Some year around the time when we started using computers, the 50s or so, someone came up with the idea of the perceptron: the brain-inspired machine that could learn! Even though the author had high hopes for this model, it sadly couldn\u0026rsquo;t learn very much. It is however very easy to implement, so let\u0026rsquo;s do it.\n# Start writing code here...\r","permalink":"http://vincentmolin.github.io/posts/perceptron/","summary":"Some year around the time when we started using computers, the 50s or so, someone came up with the idea of the perceptron: the brain-inspired machine that could learn! Even though the author had high hopes for this model, it sadly couldn\u0026rsquo;t learn very much. It is however very easy to implement, so let\u0026rsquo;s do it.\n# Start writing code here...\r","title":"A simple perceptron"},{"content":"Hej ","permalink":"http://vincentmolin.github.io/about/","summary":"about","title":"About me"}]