<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Sentiment Classification: Preprocessing Part 1 | Vincent Molin</title>

<meta name="keywords" content="[NLP, Data, Python]" />
<meta name="description" content="In this series of posts we&rsquo;re going to build a text classification model that predicts whether a comment expresses a negative or positive stance on COVID-19 vaccination. This short first installment will deal with the way in which we digest the raw comments into something a bit more workable before doing any machine-learn-y things.
Cleaning up the comments The data we have on hand are roughly 9,000 scraped comments from various sources such as YouTube, Reddit and Twitter.">
<meta name="author" content="Vincent">
<link rel="canonical" href="http://vincentmolin.github.io/posts/vacc-part1-preprocess/" />
<link href="/assets/css/stylesheet.min.42a34ddfc3f9636c886cc415318bd5be1a3dd171147875e71b23fc4063801c64.css" integrity="sha256-QqNN38P5Y2yIbMQVMYvVvho90XEUeHXnGyP8QGOAHGQ=" rel="preload stylesheet"
    as="style">

<link rel="icon" href="http://vincentmolin.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://vincentmolin.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://vincentmolin.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://vincentmolin.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="http://vincentmolin.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.80.0" />



<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
    document.querySelectorAll("mjx-container").forEach(function (x) {
      x.parentElement.classList += 'has-jax'
    })
  });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><meta property="og:title" content="Sentiment Classification: Preprocessing Part 1" />
<meta property="og:description" content="In this series of posts we&rsquo;re going to build a text classification model that predicts whether a comment expresses a negative or positive stance on COVID-19 vaccination. This short first installment will deal with the way in which we digest the raw comments into something a bit more workable before doing any machine-learn-y things.
Cleaning up the comments The data we have on hand are roughly 9,000 scraped comments from various sources such as YouTube, Reddit and Twitter." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://vincentmolin.github.io/posts/vacc-part1-preprocess/" />
<meta property="article:published_time" content="2021-02-16T00:00:00+00:00" />
<meta property="article:modified_time" content="2021-02-16T00:00:00+00:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Sentiment Classification: Preprocessing Part 1"/>
<meta name="twitter:description" content="In this series of posts we&rsquo;re going to build a text classification model that predicts whether a comment expresses a negative or positive stance on COVID-19 vaccination. This short first installment will deal with the way in which we digest the raw comments into something a bit more workable before doing any machine-learn-y things.
Cleaning up the comments The data we have on hand are roughly 9,000 scraped comments from various sources such as YouTube, Reddit and Twitter."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://vincentmolin.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Sentiment Classification: Preprocessing Part 1",
      "item": "http://vincentmolin.github.io/posts/vacc-part1-preprocess/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Sentiment Classification: Preprocessing Part 1",
  "name": "Sentiment Classification: Preprocessing Part 1",
  "description": "In this series of posts we\u0026amp;rsquo;re going to build a text classification model that predicts whether a comment expresses a negative or positive stance on COVID-19 vaccination. This ‚Ä¶",
  "keywords": [
    "[NLP", "Data", "Python]"
  ],
  "articleBody": "In this series of posts we‚Äôre going to build a text classification model that predicts whether a comment expresses a negative or positive stance on COVID-19 vaccination. This short first installment will deal with the way in which we digest the raw comments into something a bit more workable before doing any machine-learn-y things.\nCleaning up the comments The data we have on hand are roughly 9,000 scraped comments from various sources such as YouTube, Reddit and Twitter. Each comment has first been labeled individually as either 1 or 0, expressing a positive or negative stance respectively. All comments have after that also independently been labeled by at least one other annotator, as we will see. Let‚Äôs start up pandas, our data handling package of choice, and inspect some rows!\n\rimport pandas as pd\nThese comments are pretty long pd.set_option(‚Äòdisplay.max_colwidth‚Äô, None)\nTechnically not a csv but pandas don‚Äôt mind df = pd.read_csv(‚Äòvacc_train_data.tsv‚Äô, sep='\\t', names=[‚Äòtarget‚Äô,‚Äòcomment‚Äô])\nfor row in df.loc[:5,‚Äòcomment‚Äô]: print('\"' + row + ‚Äò\"\\n‚Äô)\n\r\" It is easier to fool a million people than it is to convince a million people that they have been fooled. - Mark Twain\"\r\" NATURAL IMMUNITY protected us since evolution. Do not exist anymore?\"\r\" NATURAL IMMUNITY protected us since evolution. Do not exist anymore? ? ? No one talks about it, Why? ?\"\r\" The bigest sideffect of vaccines is fewer dead children That is savage\"\r\"90% of people that get vaccinated don't get the virus Wooow what's in the vaccine then? I'm very pro vaccination, but in my opinion Covid 19 vaccine is just sweetened water.\"\r\"95.6% effective against the original strain and 85.6% effective against the variant. Excellent news. Every positive news during this pandemic is to be welcomed (no pun intended)\"\rThe goal of our preprocessing steps here is to homogenize the text data a bit. We will later in our first model only consider the words in a given sentence, without regarding the order in which they were written. To this end, punctuation becomes very irrelevant so we will just strip it, and we will also make sure that everything is in lower case. Website URLs also seem reasonable to remove.\r### Emojis\rSpecial characters that however are of interest are emojis. Consider the following three fictitious comments:\r1. `Vaccine = ‚ò†`\r2. `vaccine üòÄ` 3. `üíâü§Æü§Æ`\rStripping the emojis from these comments removes pretty important information.\rLuckily for us, all emojis have descriptions and we can just translate these using the `demoji` package! We can now write the text cleaning function. We will make use of some regexp-stuff, just roll with the punches if this looks weird.\rimport re, string import demoji #demoji.download_codes()\ndef textify_emojis(text): # Returns a dictionary with ‚Äòemoji‚Äô : ‚Äòdescription‚Äô pairs emojis = demoji.findall(text)\n# We have to slightly modify the descriptions\r# by replacing all special characters with dashes\r# and then surrounding them with \":\" and whitespace\rfor emoji, desc in emojis.items():\rdesc = ' :'+re.sub(r\"[^0-9a-zA-Z]+\", \"-\", desc )+': '\rtext = re.sub(emoji, desc, text)\rreturn text\r Function that returns nice and clean text def clean_text(text): # Make all text lowercase text = text.lower()\n# Remove links\rtext = re.sub('https?://\\S+|www\\.\\S+', '', text)\r# Replace newline with space\rtext = re.sub('\\n', ' ', text)\r# Remove all \":\" and \"-\"-characters first, we will # use these to represent emojis\rtext = re.sub(r\"[:\\-]\", '', text)\r# Replace emojis with text descriptions\rtext = textify_emojis(text)\r# Remove most non-alphanumerical characters\rtext = re.sub(r\"[^0-9a-zA-Z%:\\- ]+\", \"\", text)\r# Get rid of unnecessary whitespace\rtext = ' '.join(text.split())\r# Done!\rreturn text  text = ‚Äúüíâü§Æü§Æ‚Äù print(text, ‚Äò‚Äî‚Äô, ‚Äò\"‚Äô + clean_text(text) + ‚Äò\"')\n\rüíâü§Æü§Æ --- \":syringe: :face-vomiting: :face-vomiting:\"\rNice! Now we apply this to the whole dataset.\rdf[‚Äòcomment‚Äô] = df[‚Äòcomment‚Äô].apply(lambda x: clean_text(x))\nfor row in df.loc[:5,‚Äòcomment‚Äô]: print('\"‚Äô + row + ‚Äò\"\\n‚Äô)\n\r\"it is easier to fool a million people than it is to convince a million people that they have been fooled mark twain\"\r\"natural immunity protected us since evolution do not exist anymore\"\r\"natural immunity protected us since evolution do not exist anymore no one talks about it why\"\r\"the bigest sideffect of vaccines is fewer dead children that is savage\"\r\"90% of people that get vaccinated dont get the virus wooow whats in the vaccine then im very pro vaccination but in my opinion covid 19 vaccine is just sweetened water\"\r\"956% effective against the original strain and 856% effective against the variant excellent news every positive news during this pandemic is to be welcomed no pun intended\"\rOkay, the percentages look a bit wonky, and we didn't see any example with an encoded emoji. Looking at the raw data, row 36 contains a comment with an emoji.\rprint('\"' + df.loc[35,‚Äòcomment‚Äô] + ‚Äò\"')\n\rtrust the government :rolling-on-the-floor-laughing:\rWe might do something about the percentages later, but that's all for now! ",
  "wordCount" : "802",
  "inLanguage": "en",
  "datePublished": "2021-02-16T00:00:00Z",
  "dateModified": "2021-02-16T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Vincent"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://vincentmolin.github.io/posts/vacc-part1-preprocess/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Vincent Molin",
    "logo": {
      "@type": "ImageObject",
      "url": "http://vincentmolin.github.io/favicon.ico"
    }
  }
}
</script>





</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<noscript>
    <style type="text/css">
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://vincentmolin.github.io/" accesskey="h" title="Vincent Molin (Alt + H)">Vincent Molin</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                
                
            </span>
        </div>
        <ul id="menu" onscroll="menu_on_scroll()">
            <li>
                <a href="http://vincentmolin.github.io/about/" title="about">
                    <span>about</span>
                </a>
            </li>
            <li>
                <a href="http://vincentmolin.github.io/archives/" title="archive">
                    <span>archive</span>
                </a>
            </li>
            <li>
                <a href="http://vincentmolin.github.io/search/" title="üîç (Alt &#43; /)" accesskey=/>
                    <span>üîç</span>
                </a>
            </li></ul>
    </nav>
</header>

    <main class="main">

<article class="post-single">
  <header class="post-header">

    <h1 class="post-title">
      Sentiment Classification: Preprocessing Part 1
    </h1>
    <div class="post-meta">

February 16, 2021&nbsp;¬∑&nbsp;Vincent

</div>
  </header> 

  <div class="post-content">
<p>In this series of posts we&rsquo;re going to build a text classification model that predicts whether a comment expresses a negative or positive stance on COVID-19 vaccination. This short first installment will deal with the way in which we digest the raw comments into something a bit more workable before doing any machine-learn-y things.</p>
<h3 id="cleaning-up-the-comments">Cleaning up the comments<a hidden class="anchor" aria-hidden="true" href="#cleaning-up-the-comments">#</a></h3>
<p>The data we have on hand are roughly 9,000 scraped comments from various sources such as YouTube, Reddit and Twitter. Each comment has first been labeled individually as either 1 or 0, expressing a positive or negative stance respectively. All comments have after that also independently been labeled by at least one other annotator, as we will see. Let&rsquo;s start up <code>pandas</code>, our data handling package of choice, and inspect some rows!</p>
<pre><code>

</code></pre><p>import pandas as pd</p>
<h1 id="these-comments-are-pretty-long">These comments are pretty long<a hidden class="anchor" aria-hidden="true" href="#these-comments-are-pretty-long">#</a></h1>
<p>pd.set_option(&lsquo;display.max_colwidth&rsquo;, None)</p>
<h1 id="technically-not-a-csv-but-pandas-dont-mind">Technically not a csv but pandas don&rsquo;t mind<a hidden class="anchor" aria-hidden="true" href="#technically-not-a-csv-but-pandas-dont-mind">#</a></h1>
<p>df = pd.read_csv(&lsquo;vacc_train_data.tsv&rsquo;, sep='\t', names=[&lsquo;target&rsquo;,&lsquo;comment&rsquo;])</p>
<p>for row in df.loc[:5,&lsquo;comment&rsquo;]:
print('&quot;' + row + &lsquo;&quot;\n&rsquo;)</p>
<pre><code>
    &quot; It is easier to fool a million people than it is to convince a million people that they have been fooled. - Mark Twain&quot;
    
    &quot; NATURAL IMMUNITY  protected us since evolution. Do not exist anymore?&quot;
    
    &quot; NATURAL IMMUNITY  protected us since evolution. Do not exist anymore? ? ? No one talks about it, Why? ?&quot;
    
    &quot; The bigest sideffect of vaccines is fewer dead children  That is savage&quot;
    
    &quot;90% of people that get vaccinated don't get the virus Wooow what's in the vaccine then? I'm very pro vaccination, but in my opinion Covid 19 vaccine is just sweetened water.&quot;
    
    &quot;95.6% effective against the original strain and 85.6% effective against the variant. Excellent news. Every positive news during this pandemic is to be welcomed (no pun intended)&quot;
    
    

The goal of our preprocessing steps here is to homogenize the text data a bit. We will later in our first model only consider the words in a given sentence, without regarding the order in which they were written. To this end, punctuation becomes very irrelevant so we will just strip it, and we will also make sure that everything is in lower case. Website URLs also seem reasonable to remove.

### Emojis

Special characters that however are of interest are emojis. Consider the following three fictitious comments:

 1. `Vaccine = ‚ò†`
 2. `vaccine üòÄ` 
 3. `üíâü§Æü§Æ`

Stripping the emojis from these comments removes pretty important information.
Luckily for us, all emojis have descriptions and we can just translate these using the `demoji` package! We can now write the text cleaning function. We will make use of some regexp-stuff, just roll with the punches if this looks weird.


</code></pre><p>import re, string
import demoji
#demoji.download_codes()</p>
<p>def textify_emojis(text):
# Returns a dictionary with &lsquo;emoji&rsquo; : &lsquo;description&rsquo; pairs
emojis = demoji.findall(text)</p>
<pre><code># We have to slightly modify the descriptions
# by replacing all special characters with dashes
# and then surrounding them with &quot;:&quot; and whitespace
for emoji, desc in emojis.items():
    desc = ' :'+re.sub(r&quot;[^0-9a-zA-Z]+&quot;, &quot;-&quot;, desc )+': '
    text = re.sub(emoji, desc, text)

return text
</code></pre>
<h1 id="function-that-returns-nice-and-clean-text">Function that returns nice and clean text<a hidden class="anchor" aria-hidden="true" href="#function-that-returns-nice-and-clean-text">#</a></h1>
<p>def clean_text(text):
# Make all text lowercase
text = text.lower()</p>
<pre><code># Remove links
text = re.sub('https?://\S+|www\.\S+', '', text)

# Replace newline with space
text = re.sub('\n', ' ', text)

# Remove all &quot;:&quot; and &quot;-&quot;-characters first, we will 
# use these to represent emojis
text = re.sub(r&quot;[:\-]&quot;, '', text)

# Replace emojis with text descriptions
text = textify_emojis(text)

# Remove most non-alphanumerical characters
text = re.sub(r&quot;[^0-9a-zA-Z%:\- ]+&quot;, &quot;&quot;, text)

# Get rid of unnecessary whitespace
text = ' '.join(text.split())

# Done!
return text    
</code></pre>
<p>text = &ldquo;üíâü§Æü§Æ&rdquo;
print(text, &lsquo;&mdash;&gt;&rsquo;, &lsquo;&quot;&rsquo; + clean_text(text) + &lsquo;&quot;')</p>
<pre><code>
    üíâü§Æü§Æ ---&gt; &quot;:syringe: :face-vomiting: :face-vomiting:&quot;
    

Nice! Now we apply this to the whole dataset.


</code></pre><p>df[&lsquo;comment&rsquo;] = df[&lsquo;comment&rsquo;].apply(lambda x: clean_text(x))</p>
<p>for row in df.loc[:5,&lsquo;comment&rsquo;]:
print('&quot;&rsquo; + row + &lsquo;&quot;\n&rsquo;)</p>
<pre><code>
    &quot;it is easier to fool a million people than it is to convince a million people that they have been fooled mark twain&quot;
    
    &quot;natural immunity protected us since evolution do not exist anymore&quot;
    
    &quot;natural immunity protected us since evolution do not exist anymore no one talks about it why&quot;
    
    &quot;the bigest sideffect of vaccines is fewer dead children that is savage&quot;
    
    &quot;90% of people that get vaccinated dont get the virus wooow whats in the vaccine then im very pro vaccination but in my opinion covid 19 vaccine is just sweetened water&quot;
    
    &quot;956% effective against the original strain and 856% effective against the variant excellent news every positive news during this pandemic is to be welcomed no pun intended&quot;
    
    

Okay, the percentages look a bit wonky, and we didn't see any example with an encoded emoji. Looking at the raw data, row 36 contains a comment with an emoji.


</code></pre><p>print('&quot;' + df.loc[35,&lsquo;comment&rsquo;] + &lsquo;&quot;')</p>
<pre><code>
    trust the government :rolling-on-the-floor-laughing:
    

We might do something about the percentages later, but that's all for now! 
</code></pre>
</div>
  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://vincentmolin.github.io/tags/nlp/">[NLP</a></li>
      <li><a href="http://vincentmolin.github.io/tags/data/">Data</a></li>
      <li><a href="http://vincentmolin.github.io/tags/python/">Python]</a></li>
    </ul>
  </footer>
</article>
    </main><footer class="footer">
    <span>&copy; 2021 <a href="http://vincentmolin.github.io/">Vincent Molin</a></span>
    <span>&middot;</span>
    <span>Powered by <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a></span>
    <span>&middot;</span>
    <span>Theme <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a></span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)">
    <button class="top-link" id="top-link" type="button" accesskey="g">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z" />
        </svg>
    </button>
</a>



<script defer src="/assets/js/highlight.min.27cd435cc9ed6abb4b496581b151804f79f366c412620272bb94e2f5f598ebcc.js" integrity="sha256-J81DXMntartLSWWBsVGAT3nzZsQSYgJyu5Ti9fWY68w="
    onload="hljs.initHighlightingOnLoad();"></script>
<script>
    window.onload = function () {
        if (localStorage.getItem("menu-scroll-position")) {
            document.getElementById('menu').scrollLeft = localStorage.getItem("menu-scroll-position");
        }
    }
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

    function menu_on_scroll() {
        localStorage.setItem("menu-scroll-position", document.getElementById('menu').scrollLeft);
    }

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>

</body>

</html>
